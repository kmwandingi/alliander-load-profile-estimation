{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad56283e-95d9-4f74-9384-ddee7e72d250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hiplot as hip\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mlflow\n",
    "\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "raw_path = '../data/raw/'\n",
    "processed_path = '../data/processed/'\n",
    "sys.path.append(raw_path)\n",
    "sys.path.append(processed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7746a75d-f682-4e17-b48f-e3ec74fbdd4c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4b802b-ba18-4678-888d-72949eb9c85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Feather file...\n",
      "Feather file loaded successfully\n",
      "Time taken to load data: 11.79 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure the time taken to load the data\n",
    "start_time = time.time()\n",
    "print(\"Loading data from Feather file...\")\n",
    "try:\n",
    "    # Load Feather file using pyarrow\n",
    "    df_metered_monthly = feather.read_feather(processed_path + \"df_metered_monthly_500_balanced.feather\")\n",
    "    df_unmetered_monthly = feather.read_feather(processed_path + \"df_unmetered_monthly_500_balanced.feather\")\n",
    "    print(\"Feather file loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Feather file: {e}\")\n",
    "end_time = time.time()\n",
    "\n",
    "load_time = end_time - start_time\n",
    "print(f\"Time taken to load data: {load_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06045cf1-3587-4819-bfa1-29be58eabe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unmetered_monthly['consumption'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d40315bc-4f98-4dc0-a870-4a5d696ee197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metered_monthly['consumption'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7789af81-4aad-42a0-86c1-892a54190546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9655', '12332', '11500', '4247', '5261', '10341', '4873', '2607',\n",
       "       '769', '5562', '5445', '2212', '2081', '1241', '3515', '10938',\n",
       "       '7313', '5383', '2748', '12066', '5918', '10042', '5486', '9599',\n",
       "       '706', '1738', '7405', '3771', '966', '4964', '1531', '2068',\n",
       "       '6388', '2562', '9596', '10569', '2060', '12885', '11510', '1358',\n",
       "       '7259', '3551', '8383', '6315', '6357', '727', '7397', '5098',\n",
       "       '6975', '4731'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unmetered_monthly['RND_ID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6b7e66-c133-481f-a3e1-3c4f7238089c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Initialise, upload and Continue training the model (Later after initial training was stopped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e91f9da-f1a1-4be5-a47d-7c2ae17b7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|████████████████████████████| 123188/123188 [10:59:44<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.005716422992211756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/16 00:13:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "C:\\Users\\20235149\\AppData\\Local\\anaconda3\\Lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Epoch 2/20: 100%|█████████████████████████████| 123188/123188 [6:04:02<00:00,  5.64it/s]\n",
      "2024/07/16 06:17:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.0056994685093890675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|█████████████████████████████| 123188/123188 [6:42:16<00:00,  5.10it/s]\n",
      "2024/07/16 12:59:59 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.00568424289383066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/07/16 13:01:59 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\20235149\\AppData\\Local\\Temp\\tmp572hwhu7\\model\\data, flavor: pytorch). Fall back to return ['torch==2.1.1', 'cloudpickle==2.2.1']. Set logging level to DEBUG to see the full traceback. \n",
      "Epoch 4/20:   0%|          | 10/123188 [00:02<6:51:29,  4.99it/s]"
     ]
    }
   ],
   "source": [
    "# Energy Dataset Class Definition\n",
    "# Define the dataset class for handling energy consumption data. This will be used to load and preprocess the data.\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, data, max_consumption):\n",
    "        self.data = data\n",
    "        self.max_consumption = max_consumption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor([self.data.iloc[idx]['consumption_normalized']], dtype=torch.float32)\n",
    "        time_int = int(self.data.iloc[idx]['time_int'])\n",
    "        month_numeric = int(self.data.iloc[idx]['month'])\n",
    "        day_of_week = self.data.iloc[idx]['day_of_week']\n",
    "        day_of_year = self.data.iloc[idx]['day_of_year']\n",
    "        is_weekend = int(self.data.iloc[idx]['is_weekend'])\n",
    "        high_tariff = self.data.iloc[idx]['high_tariff']\n",
    "        consumption_monthly_avg_normalized = self.data.iloc[idx]['consumption_monthly_avg_normalized']\n",
    "        consumption_monthly_max_normalized = self.data.iloc[idx]['consumption_monthly_max_normalized']\n",
    "        \n",
    "        # Extract one-hot encoded values\n",
    "        total_bin_values = self.data.iloc[idx][self.data.columns.str.startswith('total_bin_')].to_numpy(dtype=np.float32)\n",
    "        baseload_values = self.data.iloc[idx][self.data.columns.str.startswith('baseload_')].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Combine all features\n",
    "        c = torch.tensor([time_int, month_numeric, day_of_week, day_of_year, is_weekend, high_tariff, \n",
    "                          consumption_monthly_avg_normalized, consumption_monthly_max_normalized], dtype=torch.float32)\n",
    "        c = torch.cat((c, torch.tensor(total_bin_values, dtype=torch.float32), torch.tensor(baseload_values, dtype=torch.float32)))\n",
    "        \n",
    "        return {'x': x, 'c': c}\n",
    "\n",
    "# Load and preprocess the data\n",
    "# Here we define the maximum consumption for normalization purposes.\n",
    "max_consumption = df_metered_monthly['consumption'].max()\n",
    "\n",
    "# Create datasets\n",
    "unmetered_dataset = EnergyDataset(df_unmetered_monthly, max_consumption)\n",
    "metered_dataset = EnergyDataset(df_metered_monthly, max_consumption)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "train_loader = DataLoader(metered_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(unmetered_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CVAE model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        # Define layers with layer normalization and dropout for better training stability\n",
    "        self.fc1 = nn.Linear(input_dim + condition_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc21 = nn.Linear(128, latent_dim)\n",
    "        self.fc22 = nn.Linear(128, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim + condition_dim, 128)\n",
    "        self.fc4 = nn.Linear(128, 256)\n",
    "        self.fc5 = nn.Linear(256, input_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(256)\n",
    "        self.layer_norm2 = nn.LayerNorm(128)\n",
    "        self.layer_norm3 = nn.LayerNorm(latent_dim)\n",
    "        self.layer_norm4 = nn.LayerNorm(128)\n",
    "        self.layer_norm5 = nn.LayerNorm(256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize weights for better convergence\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        # Encode the input and conditions to latent space\n",
    "        h1 = torch.relu(self.layer_norm1(self.fc1(torch.cat([x, c], dim=1))))\n",
    "        h1 = self.dropout(h1)\n",
    "        h2 = torch.relu(self.layer_norm2(self.fc2(h1)))\n",
    "        h2 = self.dropout(h2)\n",
    "        mu = self.fc21(h2)\n",
    "        logvar = self.fc22(h2)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Reparameterization trick to sample z\n",
    "        logvar_clamped = torch.clamp(logvar, min=-10, max=10)\n",
    "        std = torch.exp(0.5 * logvar_clamped)\n",
    "        eps = torch.randn_like(std) * 1e-6\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        # Decode z and conditions to reconstruct the input\n",
    "        h3 = torch.relu(self.layer_norm4(self.fc3(torch.cat([z, c], dim=1))))\n",
    "        h3 = self.dropout(h3)\n",
    "        h4 = torch.relu(self.layer_norm5(self.fc4(h3)))\n",
    "        h4 = self.dropout(h4)\n",
    "        recon = torch.sigmoid(self.fc5(h4))\n",
    "        return recon\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Forward pass through the network\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, c)\n",
    "        return recon, mu, logvar\n",
    "        \n",
    "# MMD Calculation Function\n",
    "# Define the function to compute Maximum Mean Discrepancy (MMD).   \n",
    "def compute_mmd(x, y, sigma_squares=[1, 2, 4, 8, 16]):\n",
    "    mmd = 0\n",
    "    for sigma_square in sigma_squares:\n",
    "        gamma = 1 / (2 * sigma_square)\n",
    "        K_XX = torch.exp(-gamma * torch.cdist(x, x, p=2))\n",
    "        K_YY = torch.exp(-gamma * torch.cdist(y, y, p=2))\n",
    "        K_XY = torch.exp(-gamma * torch.cdist(x, y, p=2))\n",
    "        mmd += (K_XX.mean() + K_YY.mean() - 2 * K_XY.mean())\n",
    "    return mmd\n",
    "    \n",
    "# Loss Function Definition\n",
    "# Define the loss function combining MSE, MMD, and KLD.\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)  # Reparameterization trick\n",
    "    prior_z = torch.randn_like(z)  # Sample from standard normal distribution\n",
    "\n",
    "    MMD = compute_mmd(z, prior_z)\n",
    "    MMD = torch.clamp(MMD, min=0)  # Ensure MMD is non-negative\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + MMD + KLD\n",
    "\n",
    "# Initialize model, optimizer, and set learning rate\n",
    "# Here we define the input dimensions, condition dimensions, latent space dimensions, and learning rate.\n",
    "input_dim = 1\n",
    "condition_dim = 8 + len(df_metered_monthly.columns[df_metered_monthly.columns.str.startswith('total_bin_')]) + len(df_metered_monthly.columns[df_metered_monthly.columns.str.startswith('baseload_')])\n",
    "latent_dim = 50\n",
    "lr = 1e-4\n",
    "\n",
    "# Determine the device to be used for training (GPU if available, otherwise CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the CVAE model and load pre-trained weights.\n",
    "model = CVAE(input_dim, condition_dim, latent_dim).to(device)\n",
    "model.load_state_dict(torch.load('cvae_model_epoch_3_no_attention_.pth'))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()  # Initialize GradScaler for mixed precision training\n",
    "\n",
    "num_epochs = 20  # Adjusted number of epochs\n",
    "\n",
    "# Start a parent run\n",
    "with mlflow.start_run(run_name=f\"cvae_model_layers{model.fc1.out_features}_latent_dims_{latent_dim}_conds_{condition_dim}_lr_{lr}_num_epochs_{num_epochs}_pe_attention_lstm\"):\n",
    "\n",
    "    # Log parameters in the parent run\n",
    "    mlflow.log_param(\"latent_dim\", latent_dim)\n",
    "    mlflow.log_param(\"condition_dim\", condition_dim)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with mlflow.start_run(nested=True):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            epoch_pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            for batch in train_loader:\n",
    "                x, c = batch['x'].to(device, non_blocking=True), batch['c'].to(device, non_blocking=True)\n",
    "\n",
    "                if torch.isnan(x).any() or torch.isnan(c).any():\n",
    "                    print(f\"NaN input encountered at epoch {epoch}\")\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():  # Mixed precision training\n",
    "                    recon_batch, mu, logvar = model(x, c)\n",
    "                    loss = loss_function(recon_batch, x, mu, logvar)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                epoch_pbar.update(1)\n",
    "\n",
    "            epoch_pbar.close()\n",
    "            avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "            print(f'Epoch {epoch + 1}, Loss: {avg_train_loss}')\n",
    "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "            # Save model checkpoint\n",
    "            torch.save(model.state_dict(), f'cvae_model_epoch_{epoch + 1}_no_attention_.pth')\n",
    "\n",
    "            # Log the model checkpoint with MLflow\n",
    "            mlflow.pytorch.log_model(model, f\"cvae_model_epoch_{epoch + 1}_no_attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5b810-5f34-4259-8eb7-c47da870326d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Initialise and train the model (Initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8850a-c85d-498e-bfd3-3dadedc1f97e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:52:09<00:00,  5.83it/s]\n",
      "2024/07/13 02:18:32 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.005996097603505681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20235149\\AppData\\Local\\anaconda3\\Lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Epoch 2/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:52:09<00:00,  5.83it/s]\n",
      "2024/07/13 08:11:01 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 0.006218139759772838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:52:11<00:00,  5.83it/s]\n",
      "2024/07/13 14:03:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 0.006073795790443379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:51:15<00:00,  5.85it/s]\n",
      "2024/07/13 19:54:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.005972387740824169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:50:38<00:00,  5.86it/s]\n",
      "2024/07/14 01:45:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.005896326259353148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:50:52<00:00,  5.85it/s]\n",
      "2024/07/14 07:36:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.005844758968443086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████████████████████████████| 123188/123188 [5:52:59<00:00,  5.82it/s]\n",
      "2024/07/14 13:29:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.005815199876363843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|████████████████████████████████████████████████████| 123188/123188 [6:06:00<00:00,  5.61it/s]\n",
      "2024/07/14 19:36:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.005770012054800277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|████████████████████████████████████████████████████| 123188/123188 [6:05:04<00:00,  5.62it/s]\n",
      "2024/07/15 01:41:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.005759814589106223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|███████████████████████████████████████████████████| 123188/123188 [6:04:56<00:00,  5.63it/s]\n",
      "2024/07/15 07:46:36 WARNING mlflow.utils.requirements_utils: Found torch version (2.1.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.1.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.0057251369684257975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:  58%|████████████████████████████▊                     | 70891/123188 [4:05:26<2:34:45,  5.63it/s]"
     ]
    }
   ],
   "source": [
    "# Energy Dataset Class Definition\n",
    "# Define the dataset class for handling energy consumption data. This will be used to load and preprocess the data.\n",
    "class EnergyDataset(Dataset):\n",
    "    def __init__(self, data, max_consumption):\n",
    "        self.data = data\n",
    "        self.max_consumption = max_consumption\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor([self.data.iloc[idx]['consumption_normalized']], dtype=torch.float32)\n",
    "        time_int = int(self.data.iloc[idx]['time_int'])\n",
    "        month_numeric = int(self.data.iloc[idx]['month'])\n",
    "        day_of_week = self.data.iloc[idx]['day_of_week']\n",
    "        day_of_year = self.data.iloc[idx]['day_of_year']\n",
    "        is_weekend = int(self.data.iloc[idx]['is_weekend'])\n",
    "        high_tariff = self.data.iloc[idx]['high_tariff']\n",
    "        consumption_monthly_avg_normalized = self.data.iloc[idx]['consumption_monthly_avg_normalized']\n",
    "        consumption_monthly_max_normalized = self.data.iloc[idx]['consumption_monthly_max_normalized']\n",
    "        \n",
    "        # Extract one-hot encoded values\n",
    "        total_bin_values = self.data.iloc[idx][self.data.columns.str.startswith('total_bin_')].to_numpy(dtype=np.float32)\n",
    "        baseload_values = self.data.iloc[idx][self.data.columns.str.startswith('baseload_')].to_numpy(dtype=np.float32)\n",
    "        \n",
    "        # Combine all features\n",
    "        c = torch.tensor([time_int, month_numeric, day_of_week, day_of_year, is_weekend, high_tariff, \n",
    "                          consumption_monthly_avg_normalized, consumption_monthly_max_normalized], dtype=torch.float32)\n",
    "        c = torch.cat((c, torch.tensor(total_bin_values, dtype=torch.float32), torch.tensor(baseload_values, dtype=torch.float32)))\n",
    "        \n",
    "        return {'x': x, 'c': c}\n",
    "\n",
    "# Load and preprocess the data\n",
    "# Here we define the maximum consumption for normalization purposes.\n",
    "max_consumption = df_metered_monthly['consumption'].max()\n",
    "\n",
    "# Create datasets\n",
    "unmetered_dataset = EnergyDataset(df_unmetered_monthly, max_consumption)\n",
    "metered_dataset = EnergyDataset(df_metered_monthly, max_consumption)\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "train_loader = DataLoader(metered_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(unmetered_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CVAE model\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "        # Define layers with layer normalization and dropout for better training stability\n",
    "        self.fc1 = nn.Linear(input_dim + condition_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc21 = nn.Linear(128, latent_dim)\n",
    "        self.fc22 = nn.Linear(128, latent_dim)\n",
    "        self.fc3 = nn.Linear(latent_dim + condition_dim, 128)\n",
    "        self.fc4 = nn.Linear(128, 256)\n",
    "        self.fc5 = nn.Linear(256, input_dim)\n",
    "        self.layer_norm1 = nn.LayerNorm(256)\n",
    "        self.layer_norm2 = nn.LayerNorm(128)\n",
    "        self.layer_norm3 = nn.LayerNorm(latent_dim)\n",
    "        self.layer_norm4 = nn.LayerNorm(128)\n",
    "        self.layer_norm5 = nn.LayerNorm(256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize weights for better convergence\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.01)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        # Encode the input and conditions to latent space\n",
    "        h1 = torch.relu(self.layer_norm1(self.fc1(torch.cat([x, c], dim=1))))\n",
    "        h1 = self.dropout(h1)\n",
    "        h2 = torch.relu(self.layer_norm2(self.fc2(h1)))\n",
    "        h2 = self.dropout(h2)\n",
    "        mu = self.fc21(h2)\n",
    "        logvar = self.fc22(h2)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Reparameterization trick to sample z\n",
    "        logvar_clamped = torch.clamp(logvar, min=-10, max=10)\n",
    "        std = torch.exp(0.5 * logvar_clamped)\n",
    "        eps = torch.randn_like(std) * 1e-6\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        # Decode z and conditions to reconstruct the input\n",
    "        h3 = torch.relu(self.layer_norm4(self.fc3(torch.cat([z, c], dim=1))))\n",
    "        h3 = self.dropout(h3)\n",
    "        h4 = torch.relu(self.layer_norm5(self.fc4(h3)))\n",
    "        h4 = self.dropout(h4)\n",
    "        recon = torch.sigmoid(self.fc5(h4))\n",
    "        return recon\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Forward pass through the network\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z, c)\n",
    "        return recon, mu, logvar\n",
    "        \n",
    "# MMD Calculation Function\n",
    "# Define the function to compute Maximum Mean Discrepancy (MMD).   \n",
    "def compute_mmd(x, y, sigma_squares=[1, 2, 4, 8, 16]):\n",
    "    mmd = 0\n",
    "    for sigma_square in sigma_squares:\n",
    "        gamma = 1 / (2 * sigma_square)\n",
    "        K_XX = torch.exp(-gamma * torch.cdist(x, x, p=2))\n",
    "        K_YY = torch.exp(-gamma * torch.cdist(y, y, p=2))\n",
    "        K_XY = torch.exp(-gamma * torch.cdist(x, y, p=2))\n",
    "        mmd += (K_XX.mean() + K_YY.mean() - 2 * K_XY.mean())\n",
    "    return mmd\n",
    "    \n",
    "# Loss Function Definition\n",
    "# Define the loss function combining MSE, MMD, and KLD.\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)  # Reparameterization trick\n",
    "    prior_z = torch.randn_like(z)  # Sample from standard normal distribution\n",
    "\n",
    "    MMD = compute_mmd(z, prior_z)\n",
    "    MMD = torch.clamp(MMD, min=0)  # Ensure MMD is non-negative\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return MSE + MMD + KLD\n",
    "\n",
    "# Initialize model, optimizer, and set learning rate\n",
    "# Here we define the input dimensions, condition dimensions, latent space dimensions, and learning rate.\n",
    "input_dim = 1\n",
    "condition_dim = 8 + len(df_metered_monthly.columns[df_metered_monthly.columns.str.startswith('total_bin_')]) + len(df_metered_monthly.columns[df_metered_monthly.columns.str.startswith('baseload_')])\n",
    "latent_dim = 50\n",
    "lr = 1e-4\n",
    "\n",
    "# Determine the device to be used for training (GPU if available, otherwise CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate the CVAE model and load pre-trained weights.\n",
    "model = CVAE(input_dim, condition_dim, latent_dim).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scaler = GradScaler()  # Initialize GradScaler for mixed precision training\n",
    "\n",
    "num_epochs = 20  # Adjusted number of epochs\n",
    "\n",
    "# Start a parent run\n",
    "with mlflow.start_run(run_name=f\"cvae_model_layers{model.fc1.out_features}_latent_dims_{latent_dim}_conds_{condition_dim}_lr_{lr}_num_epochs_{num_epochs}_pe_attention_lstm\"):\n",
    "\n",
    "    # Log parameters in the parent run\n",
    "    mlflow.log_param(\"latent_dim\", latent_dim)\n",
    "    mlflow.log_param(\"condition_dim\", condition_dim)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        with mlflow.start_run(nested=True):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            epoch_pbar = tqdm(total=len(train_loader), desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "            for batch in train_loader:\n",
    "                x, c = batch['x'].to(device, non_blocking=True), batch['c'].to(device, non_blocking=True)\n",
    "\n",
    "                if torch.isnan(x).any() or torch.isnan(c).any():\n",
    "                    print(f\"NaN input encountered at epoch {epoch}\")\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():  # Mixed precision training\n",
    "                    recon_batch, mu, logvar = model(x, c)\n",
    "                    loss = loss_function(recon_batch, x, mu, logvar)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                epoch_pbar.update(1)\n",
    "\n",
    "            epoch_pbar.close()\n",
    "            avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "            print(f'Epoch {epoch + 1}, Loss: {avg_train_loss}')\n",
    "            mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "            # Save model checkpoint\n",
    "            torch.save(model.state_dict(), f'cvae_model_epoch_{epoch + 1}_no_attention_.pth')\n",
    "\n",
    "            # Log the model checkpoint with MLflow\n",
    "            mlflow.pytorch.log_model(model, f\"cvae_model_epoch_{epoch + 1}_no_attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2028cec-5969-45ce-b2d0-42fe5a1690e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeceb7b3-e801-4d71-b8b2-779d9c2ab5a3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model after training\n",
    "model_path = f\"cvae_model_layers{model.fc1.out_features}-{model.fc2.out_features}_latent_{latent_dim}_num_epochs_{num_epochs}_conds_{condition_dim}_no_attention.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ff6773-94db-43e8-b96b-d1b154d23a78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "mlflow.pytorch.log_model(model, f\"cvae_model_layers{model.fc1.out_features}-{model.fc2.out_features}_latent_{latent_dim}_num_epochs_{num_epochs}_conds_{condition_dim}_no_attention\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
